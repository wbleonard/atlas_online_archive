{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas Online Archive\n",
    "\n",
    "[Atlas Online Archive](https://docs.atlas.mongodb.com/online-archive/manage-online-archive/) moves infrequently accessed immutable data from your Atlas cluster to MongoDB-managed read-only blob storage without user action. Once Atlas archives the data, you have a unified view of your Atlas and Online Archive data.\n",
    "\n",
    "In this demo we will generate 1000 IoT events for the current year. Here's an example event:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  '_id': ObjectId('5ef4ff46cf35f6a16e7f88a9'),\n",
    "  'username': 'rogerrhodes',\n",
    "  'remote_ipv4': '82.180.218.173',\n",
    "  'httpMethod': 'PATCH',\n",
    "  'hostName': 'desktop-51.freeman.net',\n",
    "  'portNum': 52048,\n",
    "  'location': {\n",
    "    'type': 'Point',\n",
    "    'coordinates': [\n",
    "      Decimal128('-158.511919'),\n",
    "      Decimal128('24.326279')\n",
    "    ]\n",
    "  },\n",
    "  'dateAccessed': datetime.datetime(2020,  6,  15,  0,  0)\n",
    "}\n",
    "```\n",
    "\n",
    "The events will be written to ```test.iot``` and Online Archive has been configured to achive documents whose ```dateAccessed``` field is older than 30 days:\n",
    "\n",
    "<img src=\"./images/online_archive.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "import settings\n",
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "from bson.decimal128 import Decimal128\n",
    "import requests\n",
    "from requests.auth import HTTPDigestAuth\n",
    "import json\n",
    "\n",
    "\n",
    "# Constants loaded from .env file\n",
    "MDB_CONNECTION = settings.MDB_CONNECTION\n",
    "MDB_CONNECTION_ARCHIVE = settings.MDB_CONNECTION_ARCHIVE\n",
    "MDB_DATABASE = settings.MDB_DATABASE\n",
    "MDB_COLLECTION = settings.MDB_COLLECTION\n",
    "NUM_DOCS = settings.NUM_DOCS\n",
    "API_PUBLIC_KEY = settings.API_PUBLIC_KEY\n",
    "API_PRIVATE_KEY = settings.API_PRIVATE_KEY\n",
    "PROJECT_ID = settings.PROJECT_ID\n",
    "CLUSTER_NAME = settings.CLUSTER_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_collection():\n",
    "    mongo_client = MongoClient(MDB_CONNECTION)\n",
    "    db = mongo_client[MDB_DATABASE]\n",
    "    return db[MDB_COLLECTION]\n",
    "\n",
    "def get_cluster_archive_collection():\n",
    "    mongo_client_archive = MongoClient(MDB_CONNECTION_ARCHIVE)\n",
    "    archive_db = mongo_client_archive[MDB_DATABASE]\n",
    "    return archive_db[MDB_COLLECTION]\n",
    "\n",
    "# Determine if archive needs to be built (or rebuilt)\n",
    "def rebuild_archive():\n",
    "    \n",
    "    # The archive doesn't exist\n",
    "    if (get_archive_id() == 0):\n",
    "        return True\n",
    "    \n",
    "    # If the archive exists, ensure there are still unarchived documents for the demo\n",
    "    my_collection = get_cluster_collection()\n",
    "    if (my_collection.count_documents({}) == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_archive_id():\n",
    "    url = \"https://cloud.mongodb.com/api/atlas/v1.0/groups/\" + PROJECT_ID +\"/clusters/\" + CLUSTER_NAME  +\"/onlineArchives\"\n",
    "    resp = requests.get(url, auth=HTTPDigestAuth(API_PUBLIC_KEY, API_PRIVATE_KEY))\n",
    "\n",
    "    if (resp.ok):\n",
    "\n",
    "        archives = json.loads(resp.content)\n",
    "        #print (\"There are {0} online archive(s)\".format(len(archives)))\n",
    "\n",
    "        for archive in archives:\n",
    "\n",
    "            # There can only be one online archive per collection.\n",
    "            if (archive['dbName'] == MDB_DATABASE and archive['collName'] == MDB_COLLECTION):\n",
    "                return archive['_id']\n",
    "            else:\n",
    "                return 0;\n",
    "        \n",
    "        return 0;\n",
    "\n",
    "    else:\n",
    "        print(resp)\n",
    "        \n",
    "def get_archive_state(id):\n",
    "        \n",
    "    url = \"https://cloud.mongodb.com/api/atlas/v1.0/groups/\" + PROJECT_ID +\"/clusters/\" + CLUSTER_NAME  +\"/onlineArchives/\" + str(id)\n",
    "    resp = requests.get(url, auth=HTTPDigestAuth(API_PUBLIC_KEY, API_PRIVATE_KEY))\n",
    "\n",
    "    if (resp.ok):\n",
    "        return resp.json()['state']\n",
    "\n",
    "    else:\n",
    "        print(resp)\n",
    "        \n",
    "def delete_archive(archive_id):\n",
    "\n",
    "    url = \"https://cloud.mongodb.com/api/atlas/v1.0/groups/\" + PROJECT_ID +\"/clusters/\" + CLUSTER_NAME  +\"/onlineArchives/\" + archive_id\n",
    "    resp = requests.delete(url, auth=HTTPDigestAuth(API_PUBLIC_KEY, API_PRIVATE_KEY))\n",
    "\n",
    "    if (resp.ok):\n",
    "        wait = 60\n",
    "        print (\"Giving archive \" + str(wait) + \" seconds to delete.\", end=\"\")\n",
    "        for x in range(wait):            \n",
    "            time.sleep(1) # Allowance for deletion to complete\n",
    "            print(\".\", end=\"\")\n",
    "            x = x+1\n",
    "        print(\"Deleted existing archive\")  \n",
    "\n",
    "    else:\n",
    "        print(resp) \n",
    "        \n",
    "def create_archive():\n",
    "    date_field = \"dateAccessed\"\n",
    "\n",
    "    url = \"https://cloud.mongodb.com/api/atlas/v1.0/groups/\" + PROJECT_ID +\"/clusters/\" + CLUSTER_NAME  +\"/onlineArchives\"\n",
    "\n",
    "    data = {\n",
    "            \"dbName\": MDB_DATABASE,\n",
    "            \"collName\": MDB_COLLECTION,\n",
    "            \"partitionFields\": [\n",
    "                  {\n",
    "                          \"fieldType\": \"string\",\n",
    "                          \"fieldName\": \"userName\",\n",
    "                          \"order\": 0\n",
    "                  },                  \n",
    "                  {\n",
    "                          \"fieldType\": \"string\",\n",
    "                          \"fieldName\": \"httpMethod\",\n",
    "                          \"order\": 0\n",
    "                  }],    \n",
    "            \"criteria\": {\n",
    "                  \"dateField\": date_field,\n",
    "                  \"expireAfterDays\": 30\n",
    "              }\n",
    "    }\n",
    "    headers = {\"content-type\":\"application/json\"}\n",
    "\n",
    "    resp = requests.post(url, auth=HTTPDigestAuth(API_PUBLIC_KEY, API_PRIVATE_KEY), json=data, headers=headers)\n",
    "\n",
    "    if resp.ok:\n",
    "        print(\"Archive Created\")\n",
    "        #print(resp.json())\n",
    "        \n",
    "    elif resp.status == 500:\n",
    "        print(\"The server returned an error, but the archive was probably created. Please validate using the Atlas UI.\")\n",
    "\n",
    "    else:\n",
    "       print(resp)    \n",
    "    \n",
    "\n",
    "# Wait for archive to become active\n",
    "def wait_for_active_archive(archive_id):\n",
    "    \n",
    "    state = get_archive_state(archive_id) \n",
    "    if (state == 'ACTIVE'):\n",
    "        print(state)\n",
    "    else:\n",
    "        print (\"Waiting for archive to build\",  end=\"\")\n",
    "\n",
    "    while state != 'ACTIVE':\n",
    "        state = get_archive_state(archive_id) \n",
    "        print(\".\", end=\"\")    \n",
    "        if (state == 'ACTIVE'):\n",
    "            print (\"\\n\" + state)\n",
    "            break;\n",
    "        else:\n",
    "            time.sleep(1)    \n",
    "            \n",
    "def generate_events():\n",
    "    fake = Faker()\n",
    "\n",
    "    # Start script\n",
    "    startTs = time.gmtime()\n",
    "    start = timer()\n",
    "    print(\"================================\")\n",
    "    print(\"   Generating Sample IoT Data   \")\n",
    "    print(\"================================\")\n",
    "    print(\"\\nStarting \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", startTs) + \"\\n\")\n",
    "\n",
    "    print('NUM DOCS TO GENERATE: ' + str(NUM_DOCS))\n",
    "\n",
    "    mongo_client = MongoClient(MDB_CONNECTION)\n",
    "    db = mongo_client[MDB_DATABASE]\n",
    "    my_collection = db[MDB_COLLECTION]\n",
    "\n",
    "    # Remove the existing documents (don't drop the collection from underneath the archive)\n",
    "    my_collection.delete_many({})\n",
    "\n",
    "    for index in range(int(NUM_DOCS)):\n",
    "        # create timestamp\n",
    "        fake_timestamp = fake.date_this_year()\n",
    "\n",
    "        # Define IoT Document\n",
    "        my_iot_document = {\n",
    "            \"username\": fake.user_name(),\n",
    "            \"remote_ipv4\": fake.ipv4(),\n",
    "            \"httpMethod\": fake.http_method(),\n",
    "            \"hostName\": fake.hostname(),\n",
    "            \"portNum\": fake.port_number(),\n",
    "            \"location\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [\n",
    "                        Decimal128(fake.longitude()),\n",
    "                        Decimal128(fake.latitude())\n",
    "                    ]\n",
    "            },\n",
    "            \"dateAccessed\": datetime.datetime(fake_timestamp.year, fake_timestamp.month, fake_timestamp.day)\n",
    "        }\n",
    "        # print(my_iot_document)\n",
    "        print(\".\", end=\"\")\n",
    "        my_collection.insert_one(my_iot_document)\n",
    "\n",
    "    # Indicate end of script\n",
    "    end = timer()\n",
    "    endTs = time.gmtime()\n",
    "    print(\"\\nEnding \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", endTs))\n",
    "    print('===============================')\n",
    "    print('Total Time Elapsed (in seconds): ' + str(end - start))\n",
    "    print('===============================')    \n",
    "              \n",
    "def get_archive_date():\n",
    "    return datetime.datetime.now() - timedelta(30)\n",
    "\n",
    "def print_row(count, source):\n",
    "    formatted_count = str(count).rjust(5)\n",
    "    print(\" %-10s %45s\" % (formatted_count, source))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive Test and Setup\n",
    "\n",
    "If the cluster still contains unarchived events, the demo's good to go. Otherwise, the archive will be rebuilt. This way the demo's consistent and there's always 1000 documents to work with across the cluster and the archive. set ```force_rebuilt = TRUE``` to start from scratch. Note, if attempting to demo this live, it can take several minutes for documents to archive after the archive is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No archive to delete\n",
      "<Response [500]>\n",
      "Waiting for archive to build......................................................................................\n",
      "ACTIVE\n",
      "================================\n",
      "   Generating Sample IoT Data   \n",
      "================================\n",
      "\n",
      "Starting 2020-07-15 14:16:59\n",
      "\n",
      "NUM DOCS TO GENERATE: 1000\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "Ending 2020-07-15 14:17:39\n",
      "===============================\n",
      "Total Time Elapsed (in seconds): 39.91534021999999\n",
      "===============================\n"
     ]
    }
   ],
   "source": [
    "force_rebuild = True\n",
    "\n",
    "if (rebuild_archive() or force_rebuild):    \n",
    "\n",
    "    ## Delete Existing Archive \n",
    "    archive_id = get_archive_id()\n",
    "    if (archive_id):\n",
    "            delete_archive(archive_id)\n",
    "    else:\n",
    "        print(\"No archive to delete\")\n",
    "\n",
    "    ## Create New Archive\n",
    "    create_archive()\n",
    "\n",
    "    # Wait for archive to become active\n",
    "    archive_id = get_archive_id()\n",
    "    wait_for_active_archive(archive_id)\n",
    "    \n",
    "    # Generate events\n",
    "    generate_events()\n",
    "    \n",
    "else:\n",
    "    print (\"Demo is ready\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Query\n",
    "Query the cluster's document count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for documents to archive\n",
      "................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "my_collection = get_cluster_collection()\n",
    "\n",
    "cluster_count = my_collection.count_documents({})\n",
    "\n",
    "if cluster_count == 1000:\n",
    "    print (\"Waiting for documents to archive:\")\n",
    "    \n",
    "    while cluster_count == 1000:\n",
    "        print(\".\", end=\"\")\n",
    "        time.sleep(1)\n",
    "\n",
    "print(\"Archive has begun:\")\n",
    "cluster_count = my_collection.count_documents({})\n",
    "print_row (cluster_count, \"Total number of documents in the Atlas Cluster\")\n",
    "\n",
    "# While the document count is shrinking\n",
    "while my_collection.count_documents({}) < cluster_count:\n",
    "    print_row (count, \"Total number of documents in the Atlas Cluster\")\n",
    "    time.sleep(1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and Online Archive\n",
    "\n",
    "While the Atlas Cluster has some subset of the documents, there are still 1000 documents across the cluster and archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the Cluster and Online Archive\n",
    "archive_date = get_archive_date()\n",
    "my_collection = get_cluster_collection()\n",
    "my_collection_archive = get_cluster_archive_collection()\n",
    "\n",
    "cluster_count = my_collection.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "cluster_archive_count = my_collection_archive.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "\n",
    "print(\"Archive date (30 days ago): \" + str(archive_date.date()))\n",
    "print('')\n",
    "print_row(my_collection.count_documents({}), \"Total number of documents in the Atlas Cluster\")\n",
    "print_row(cluster_count, \"Total number of documents in the Atlas Cluster older than 30 days\")\n",
    "print_row(cluster_archive_count, \"Total number of documents across the Atlas Cluster and the Online Archive older than 30 days\")\n",
    "print('------')\n",
    "print_row(my_collection_archive.count_documents({}), \"Total number of documents across the Atlas Cluster and Online Archive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
