{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas Online Archive\n",
    "\n",
    "[Atlas Online Archive](https://docs.atlas.mongodb.com/online-archive/manage-online-archive/) moves infrequently accessed immutable data from your Atlas cluster to MongoDB-managed read-only blob storage without user action. Once Atlas archives the data, you have a unified view of your Atlas and Online Archive data.\n",
    "\n",
    "In this demo we will generate 1000 IoT events for the current year. Here's an example event:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  '_id': ObjectId('5ef4ff46cf35f6a16e7f88a9'),\n",
    "  'username': 'rogerrhodes',\n",
    "  'remote_ipv4': '82.180.218.173',\n",
    "  'httpMethod': 'PATCH',\n",
    "  'hostName': 'desktop-51.freeman.net',\n",
    "  'portNum': 52048,\n",
    "  'location': {\n",
    "    'type': 'Point',\n",
    "    'coordinates': [\n",
    "      Decimal128('-158.511919'),\n",
    "      Decimal128('24.326279')\n",
    "    ]\n",
    "  },\n",
    "  'dateAccessed': datetime.datetime(2020,  6,  15,  0,  0)\n",
    "}\n",
    "```\n",
    "\n",
    "The events will be written to ```test.iot``` and Online Archive has been configured to achive documents whose ```dateAccessed``` field is older than 30 days:\n",
    "\n",
    "<img src=\"./images/online_archive.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!sudo -E pip install --upgrade pip\n",
    "!sudo -E pip install python-dotenv\n",
    "!sudo -E pip install faker\n",
    "!sudo -E pip install --upgrade pymongo\n",
    "!sudo -E pip install dnspython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Settings loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import time\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "import settings\n",
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "from bson.decimal128 import Decimal128\n",
    "import requests\n",
    "from requests.auth import HTTPDigestAuth\n",
    "import json\n",
    "\n",
    "\n",
    "# Constants loaded from .env file\n",
    "MDB_CONNECTION = settings.MDB_CONNECTION\n",
    "MDB_CONNECTION_ARCHIVE = settings.MDB_CONNECTION_ARCHIVE\n",
    "PEM_FILE = settings.PEM_FILE\n",
    "MDB_DATABASE = settings.MDB_DATABASE\n",
    "MDB_COLLECTION = settings.MDB_COLLECTION\n",
    "NUM_DOCS = settings.NUM_DOCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_collection():\n",
    "    mongo_client = MongoClient(MDB_CONNECTION, ssl_certfile=PEM_FILE)\n",
    "    db = mongo_client[MDB_DATABASE]\n",
    "    return db[MDB_COLLECTION]\n",
    "\n",
    "def get_cluster_archive_collection():\n",
    "    mongo_client_archive = MongoClient(MDB_CONNECTION_ARCHIVE)\n",
    "    archive_db = mongo_client_archive[MDB_DATABASE]\n",
    "    return archive_db[MDB_COLLECTION]\n",
    "\n",
    "# Verity records still remain in the cluster.\n",
    "def refresh_needed():\n",
    "    \n",
    "    # Ensure there are still unarchived documents for the demo\n",
    "    my_collection = get_cluster_collection()\n",
    "    records = my_collection.count_documents({})\n",
    "    if ( records == 0):\n",
    "        print ('There are no records in the cluster')\n",
    "        return True\n",
    "    else:\n",
    "        print ('There are ' + str(records) + ' documents in the cluster')\n",
    "        return False\n",
    "            \n",
    "def generate_events():\n",
    "    fake = Faker()\n",
    "\n",
    "    # Start script\n",
    "    startTs = time.gmtime()\n",
    "    start = timer()\n",
    "    print(\"================================\")\n",
    "    print(\"   Generating Sample IoT Data   \")\n",
    "    print(\"================================\")\n",
    "    print(\"\\nStarting \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", startTs) + \"\\n\")\n",
    "\n",
    "    print('NUM DOCS TO GENERATE: ' + str(NUM_DOCS))\n",
    "\n",
    "    my_collection = get_cluster_collection()\n",
    "\n",
    "    # Remove the existing documents (don't drop the collection from underneath the archive)\n",
    "    my_collection.delete_many({})\n",
    "\n",
    "    for index in range(int(NUM_DOCS)):\n",
    "        # create timestamp\n",
    "        fake_timestamp = fake.date_this_year()\n",
    "\n",
    "        # Define IoT Document\n",
    "        my_iot_document = {\n",
    "            \"username\": fake.user_name(),\n",
    "            \"remote_ipv4\": fake.ipv4(),\n",
    "            \"httpMethod\": fake.http_method(),\n",
    "            \"hostName\": fake.hostname(),\n",
    "            \"portNum\": fake.port_number(),\n",
    "            \"location\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [\n",
    "                        Decimal128(fake.longitude()),\n",
    "                        Decimal128(fake.latitude())\n",
    "                    ]\n",
    "            },\n",
    "            \"dateAccessed\": datetime.datetime(fake_timestamp.year, fake_timestamp.month, fake_timestamp.day)\n",
    "        }\n",
    "        # print(my_iot_document)\n",
    "        print(\".\", end=\"\")\n",
    "        my_collection.insert_one(my_iot_document)\n",
    "\n",
    "    # Indicate end of script\n",
    "    end = timer()\n",
    "    endTs = time.gmtime()\n",
    "    print(\"\\nEnding \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", endTs))\n",
    "    print('===============================')\n",
    "    print('Total Time Elapsed (in seconds): ' + str(end - start))\n",
    "    print('===============================')    \n",
    "              \n",
    "def get_archive_date():\n",
    "    return datetime.datetime.now() - datetime.timedelta(30)\n",
    "\n",
    "def print_row(count, source):\n",
    "    formatted_count = str(count).rjust(5)\n",
    "    print(\" %-10s %45s\" % (formatted_count, source))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive Test and Setup\n",
    "\n",
    "If the cluster still contains unarchived events, the demo's good to go. Otherwise, the archive should be rebuilt. To keep the demo consistent, we'll keep 1000 records in the collection, across the cluster and the archive. \n",
    "\n",
    "To rebuild the demo, delete and recreate the Online Archive (the archive itself is immutable).\n",
    "\n",
    "<img src=\"./images/config_archive.png\">\n",
    "\n",
    "Then open a new cell and run the ```generate_events()``` funcction. *Note, I played with using a flag that could be set to call this function, but I kept forgetting to unset the flag and generating more events. Forcing the creatiion of cell for this uncommon event should prevent this.*\n",
    "\n",
    "Note, if attempting to demo this live, it can take several minutes for documents to archive after the archive is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 documents in the cluster\n",
      "Online Archive is ready to demo!\n"
     ]
    }
   ],
   "source": [
    "if refresh_needed(): \n",
    "    \n",
    "    print ('\\nRecreate the Online Archive:')\n",
    "    print ('   Step 1: Delete and Online Archive.')\n",
    "    print ('   Step 2: Return here, open a new cell and run the generate_events() function.')\n",
    "    print ('   Step 3: Create the Online Archive.')\n",
    "    \n",
    "else:\n",
    "    print (\"Online Archive is ready to demo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Query\n",
    "Query the cluster's document count. In the case of a demo refresh, the function will wait until the archive of documents has been completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = get_cluster_collection()\n",
    "\n",
    "archive_date = get_archive_date()\n",
    "query = {'dateAccessed':{'$lt': archive_date}}\n",
    "\n",
    "docs_waiting_for_archive = my_collection.count_documents(query)\n",
    "\n",
    "if docs_waiting_for_archive > 0:\n",
    "    print (str(docs_waiting_for_archive) + \" documents remaining to be archived\")   \n",
    "\n",
    "    while docs_waiting_for_archive > 0:\n",
    "        current_docs_waiting_for_archive = my_collection.count_documents(query)\n",
    "        if current_docs_waiting_for_archive < docs_waiting_for_archive:\n",
    "            print (str(current_docs_waiting_for_archive) + \" documents remaining to be archived\")\n",
    "            docs_waiting_for_archive = current_docs_waiting_for_archive\n",
    "        else:\n",
    "            print(\".\", end=\"\")    \n",
    "        time.sleep(1)\n",
    "else:\n",
    "    print(\"Archive complete. \" + str(my_collection.count_documents({})) + \" documents remain in the Atlas Cluster\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and Online Archive\n",
    "\n",
    "While the Atlas Cluster has some subset of the documents, there are still 1000 documents across the cluster and archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the Cluster and Online Archive\n",
    "archive_date = get_archive_date()\n",
    "my_collection = get_cluster_collection()\n",
    "my_collection_archive = get_cluster_archive_collection()\n",
    "\n",
    "cluster_count = my_collection.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "cluster_archive_count = my_collection_archive.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "\n",
    "print(\"Archive date (30 days ago): \" + str(archive_date.date()))\n",
    "print('')\n",
    "print_row(my_collection.count_documents({}), \"Total number of documents in the Atlas Cluster\")\n",
    "print_row(cluster_count, \"Total number of documents in the Atlas Cluster older than 30 days\")\n",
    "print_row(cluster_archive_count, \"Total number of documents across the Atlas Cluster and the Online Archive older than 30 days\")\n",
    "print('------')\n",
    "print_row(my_collection_archive.count_documents({}), \"Total number of documents across the Atlas Cluster and Online Archive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
