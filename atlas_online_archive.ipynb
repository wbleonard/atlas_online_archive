{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atlas Online Archive\n",
    "\n",
    "[Atlas Online Archive](https://docs.atlas.mongodb.com/online-archive/manage-online-archive/) moves infrequently accessed immutable data from your Atlas cluster to MongoDB-managed read-only blob storage without user action. Once Atlas archives the data, you have a unified view of your Atlas and Online Archive data.\n",
    "\n",
    "<img src=\"./images/online_archive_architecture.png\">\n",
    "\n",
    "In this demo we will generate 1000 IoT events for the current year. Here's an example event:\n",
    "\n",
    "```JSON\n",
    "{\n",
    "  '_id': ObjectId('5ef4ff46cf35f6a16e7f88a9'),\n",
    "  'username': 'rogerrhodes',\n",
    "  'remote_ipv4': '82.180.218.173',\n",
    "  'httpMethod': 'PATCH',\n",
    "  'hostName': 'desktop-51.freeman.net',\n",
    "  'portNum': 52048,\n",
    "  'location': {\n",
    "    'type': 'Point',\n",
    "    'coordinates': [\n",
    "      Decimal128('-158.511919'),\n",
    "      Decimal128('24.326279')\n",
    "    ]\n",
    "  },\n",
    "  'dateAccessed': datetime.datetime(2020,  6,  15,  0,  0)\n",
    "}\n",
    "```\n",
    "\n",
    "The events will be written to ```test.iot``` and Online Archive has been configured to achive documents whose ```dateAccessed``` field is older than 30 days:\n",
    "\n",
    "<img src=\"./images/online_archive.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!sudo -E pip install --upgrade pip\n",
    "!sudo -E pip install python-dotenv\n",
    "!sudo -E pip install faker\n",
    "!sudo -E pip install --upgrade pymongo\n",
    "!sudo -E pip install dnspython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "import settings\n",
    "from pymongo import MongoClient\n",
    "from faker import Faker\n",
    "from bson.decimal128 import Decimal128\n",
    "import requests\n",
    "from requests.auth import HTTPDigestAuth\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "\n",
    "# Constants loaded from .env file\n",
    "MDB_CONNECTION = settings.MDB_CONNECTION\n",
    "MDB_CONNECTION_ARCHIVE = settings.MDB_CONNECTION_ARCHIVE\n",
    "PEM_FILE = settings.PEM_FILE\n",
    "MDB_DATABASE = settings.MDB_DATABASE\n",
    "MDB_COLLECTION = settings.MDB_COLLECTION\n",
    "NUM_DOCS = settings.NUM_DOCS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cluster_collection():\n",
    "    mongo_client = MongoClient(MDB_CONNECTION, ssl_certfile=PEM_FILE)\n",
    "    db = mongo_client[MDB_DATABASE]\n",
    "    return db[MDB_COLLECTION]\n",
    "\n",
    "def get_cluster_archive_collection():\n",
    "    mongo_client_archive = MongoClient(MDB_CONNECTION_ARCHIVE)\n",
    "    archive_db = mongo_client_archive[MDB_DATABASE]\n",
    "    return archive_db[MDB_COLLECTION]\n",
    "\n",
    "# Verity records still remain in the cluster.\n",
    "def refresh_needed():\n",
    "    \n",
    "    # Ensure there are still unarchived documents for the demo\n",
    "    my_collection = get_cluster_collection()\n",
    "    records = my_collection.count_documents({})\n",
    "    if ( records == 0):\n",
    "        print ('There are no records in the cluster')\n",
    "        return True\n",
    "    else:\n",
    "        print ('There are ' + str(records) + ' documents in the cluster')\n",
    "        return False\n",
    "            \n",
    "def generate_events():\n",
    "    fake = Faker()\n",
    "\n",
    "    # Start script\n",
    "    startTs = time.gmtime()\n",
    "    start = timer()\n",
    "    print(\"================================\")\n",
    "    print(\"   Generating Sample IoT Data   \")\n",
    "    print(\"================================\")\n",
    "    print(\"\\nStarting \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", startTs) + \"\\n\")\n",
    "\n",
    "    print('NUM DOCS TO GENERATE: ' + str(NUM_DOCS))\n",
    "\n",
    "    my_collection = get_cluster_collection()\n",
    "\n",
    "    # Remove the existing documents (don't drop the collection from underneath the archive)\n",
    "    my_collection.delete_many({})\n",
    "\n",
    "    for index in range(int(NUM_DOCS)):\n",
    "        # create timestamp\n",
    "        fake_timestamp = fake.date_this_year()\n",
    "\n",
    "        # Define IoT Document\n",
    "        my_iot_document = {\n",
    "            \"username\": fake.user_name(),\n",
    "            \"remote_ipv4\": fake.ipv4(),\n",
    "            \"httpMethod\": fake.http_method(),\n",
    "            \"hostName\": fake.hostname(),\n",
    "            \"portNum\": fake.port_number(),\n",
    "            \"location\": {\n",
    "                    \"type\": \"Point\",\n",
    "                    \"coordinates\": [\n",
    "                        Decimal128(fake.longitude()),\n",
    "                        Decimal128(fake.latitude())\n",
    "                    ]\n",
    "            },\n",
    "            \"dateAccessed\": datetime.datetime(fake_timestamp.year, fake_timestamp.month, fake_timestamp.day)\n",
    "        }\n",
    "        # print(my_iot_document)\n",
    "        print(\".\", end=\"\")\n",
    "        my_collection.insert_one(my_iot_document)\n",
    "\n",
    "    # Indicate end of script\n",
    "    end = timer()\n",
    "    endTs = time.gmtime()\n",
    "    print(\"\\nEnding \" + time.strftime(\"%Y-%m-%d %H:%M:%S\", endTs))\n",
    "    print('===============================')\n",
    "    print('Total Time Elapsed (in seconds): ' + str(end - start))\n",
    "    print('===============================')    \n",
    "              \n",
    "\n",
    "# Use after a new archive is created to verify the data has been archived.\n",
    "def wait_for_data_to_archive():\n",
    "    my_collection = get_cluster_collection()\n",
    "\n",
    "    archive_date = get_archive_date()\n",
    "    query = {'dateAccessed':{'$lt': archive_date}}\n",
    "\n",
    "    docs_waiting_for_archive = my_collection.count_documents(query)\n",
    "\n",
    "    if docs_waiting_for_archive > 0:\n",
    "        print (str(docs_waiting_for_archive) + \" documents remaining to be archived\")   \n",
    "\n",
    "        while docs_waiting_for_archive > 0:\n",
    "            current_docs_waiting_for_archive = my_collection.count_documents(query)\n",
    "            if current_docs_waiting_for_archive < docs_waiting_for_archive:\n",
    "                print (str(current_docs_waiting_for_archive) + \" documents remaining to be archived\")\n",
    "                docs_waiting_for_archive = current_docs_waiting_for_archive\n",
    "            else:\n",
    "                print(\".\", end=\"\")    \n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(\"Archive complete. \" + str(my_collection.count_documents({})) + \" documents remain in the Atlas Cluster\")    \n",
    "        \n",
    "def get_archive_date():\n",
    "    return datetime.datetime.now() - datetime.timedelta(30)\n",
    "\n",
    "def print_row(count, source):\n",
    "    formatted_count = str(count).rjust(5)\n",
    "    print(\" %-10s %45s\" % (formatted_count, source))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archive Test and Setup\n",
    "\n",
    "If the cluster still contains unarchived events, the demo's good to go. Otherwise, the archive should be rebuilt. To keep the demo consistent, we'll keep 1000 records in the collection, across the cluster and the archive. \n",
    "\n",
    "To rebuild the demo, delete and recreate the Online Archive (the archive itself is immutable).\n",
    "\n",
    "<img src=\"./images/config_archive.png\">\n",
    "\n",
    "Then open a new cell and run the ```generate_events()``` funcction. *Note, I played with using a flag that could be set to call this function, but I kept forgetting to unset the flag and generating more events. Forcing the creatiion of cell for this uncommon event should prevent this.*\n",
    "\n",
    "Note, if attempting to demo this live, it can take several minutes for documents to archive after the archive is created. You can run the ```wait_for_data_to_archive()``` function to verify the demo is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if refresh_needed(): \n",
    "    \n",
    "    print ('\\nRecreate the Online Archive:')\n",
    "    print ('   Step 1: Delete and Online Archive.')\n",
    "    print ('   Step 2: Return here, open a new cell and run the generate_events() function.')\n",
    "    print ('   Step 3: Create the Online Archive.')\n",
    "    \n",
    "else:\n",
    "    print (\"Online Archive is ready to demo!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Query\n",
    "\n",
    "With our archive configuration, the cluster only contains 30 days of events. Let's validate that's true by counting the number of documents older than 30 days..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = get_cluster_collection()\n",
    "\n",
    "archive_date = get_archive_date()\n",
    "query = {'dateAccessed':{'$lt': archive_date}}\n",
    "\n",
    "print ('All documents older than ' + str(archive_date.date()) + ' have been archived.\\n')\n",
    "\n",
    "print ('A query for documents older than ' + str(archive_date.date()) + ' should return zero.\\n')\n",
    "\n",
    "query = {'dateAccessed':{'$lt': archive_date}}\n",
    "doc_count = my_collection.count_documents(query)\n",
    "print (str(doc_count) + ' documents returned from the query\\n')\n",
    "\n",
    "print (\"Here's an example document:\\n\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(my_collection.find_one())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster and Online Archive\n",
    "\n",
    "Atlas now provides two connection options: **Connect to Cluster** and **Connect to Cluster and Online Archive**:\n",
    "\n",
    "<img src=\"./images/connection_options.png\">\n",
    "\n",
    "Connecting to the Cluster and Online Archive will give you a unified view of the documents. Let's see..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the Cluster and Online Archive\n",
    "my_collection_archive = get_cluster_archive_collection()\n",
    "\n",
    "archive_date = get_archive_date()\n",
    "query = {'dateAccessed':{'$lt': archive_date}}\n",
    "\n",
    "print ('All documents older than ' + str(archive_date.date()) + ' have been archived.\\n')\n",
    "\n",
    "print ('A query for documents older than ' + str(archive_date.date()) + ' should return documents.\\n')\n",
    "\n",
    "doc_count = my_collection_archive.count_documents(query)\n",
    "print (str(doc_count) + ' documents returned from the query\\n')\n",
    "\n",
    "print (\"Here's an example document from the archive:\\n\")\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(my_collection_archive.find_one(query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Math Across the Cluster an Online Archive\n",
    "While the Atlas Cluster has some subset of the documents, there are still 1000 documents across the cluster and archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish a connection to the Cluster and Online Archive\n",
    "my_collection_archive = get_cluster_archive_collection()\n",
    "\n",
    "archive_date = get_archive_date()\n",
    "query = {'dateAccessed':{'$lt': archive_date}}\n",
    "\n",
    "cluster_count = my_collection.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "cluster_archive_count = my_collection_archive.count_documents({'dateAccessed':{'$lt': archive_date}})\n",
    "\n",
    "print(\"Archive date (30 days ago): \" + str(archive_date.date()))\n",
    "print('')\n",
    "print_row(my_collection.count_documents({}), \"Total number of documents in the Atlas Cluster\")\n",
    "print_row(cluster_count, \"Total number of documents in the Atlas Cluster older than 30 days\")\n",
    "print_row(cluster_archive_count, \"Total number of documents across the Atlas Cluster and the Online Archive older than 30 days\")\n",
    "print('------')\n",
    "print_row(my_collection_archive.count_documents({}), \"Total number of documents across the Atlas Cluster and Online Archive\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
